{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aradhyakapil/Food-label-analyzer/blob/main/food_label_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy1RDXn2GhLa",
        "outputId": "9b0934b5-0533-423c-d737-7f431fb6fe7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG7mJl4HDKJb"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install easyocr\n",
        "!pip install streamlit\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install Pillow\n",
        "!pip install numpy\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install sentencepiece protobuf scipy\n",
        "\n",
        "# Install Streamlit dependencies for running in Colab\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef7mvUHFNaiD"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "\n",
        "# -----------------------------------------\n",
        "# Setup logging & clear GPU cache\n",
        "# -----------------------------------------\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    logger.info(\n",
        "        f\"GPU memory before loading: {props.total_memory/1e9:.2f} GB total, \"\n",
        "        f\"{torch.cuda.memory_allocated()/1e9:.2f} GB allocated, \"\n",
        "        f\"{torch.cuda.memory_reserved()/1e9:.2f} GB reserved\"\n",
        "    )\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1) Load CSV data\n",
        "# -----------------------------------------\n",
        "PATH = \"/content/drive/MyDrive/ingredients.csv\"\n",
        "if not os.path.exists(PATH):\n",
        "    raise FileNotFoundError(f\"{PATH} not found\")\n",
        "df = pd.read_csv(PATH)\n",
        "df.columns = df.columns.str.strip()\n",
        "logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2) Build prompt/response pairs\n",
        "# -----------------------------------------\n",
        "def create_prompt_response(row):\n",
        "    prompt = (\n",
        "        f\"Ingredient: {row['ingredient_name']}, Category: {row['category']}, \"\n",
        "        f\"Glycemic Index: {row['gi_value']}, Carbs per 100g: {row['carbs_per_100g']}, \"\n",
        "        f\"Sodium per 100g: {row['sodium_per_100g']}, Health Concerns: Diabetic Risk, Hypertension Risk, Pregnancy Risk\\n\"\n",
        "        \"Analyze this ingredient for health profiles with potential concerns.\"\n",
        "    )\n",
        "    response = (\n",
        "        \"{\\n\"\n",
        "        f'  \"risk_level\": \"{row[\"general_risk_level\"]}\",\\n'\n",
        "        f'  \"effects\": [\"{row[\"general_risk_reason\"]}\"],\\n'\n",
        "        '  \"recommendations\": [\"Monitor consumption based on your health profile.\"]\\n'\n",
        "        \"}\\n\\n\"\n",
        "        f\"For diabetics - Risk Level: {row['diabetic_risk_level']}\\n\"\n",
        "        f\"Effects: {row['diabetic_risk_reason']}\\n\"\n",
        "        f\"For hypertension - Risk Level: {row['hypertension_risk_level']}\\n\"\n",
        "        f\"Effects: {row['hypertension_risk_reason']}\\n\"\n",
        "        f\"For pregnancy - Risk Level: {row['pregnancy_risk_level']}\\n\"\n",
        "        f\"Effects: {row['pregnancy_risk_reason']}\"\n",
        "    )\n",
        "    return {'input': prompt, 'output': response}\n",
        "\n",
        "dataset = Dataset.from_list(df.apply(create_prompt_response, axis=1).tolist())\n",
        "logger.info(f\"Prepared {len(dataset)} examples\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3) Load & quantize model\n",
        "# -----------------------------------------\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4) Apply LoRA + enable grads on inputs\n",
        "# -----------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Disable kv-cache so gradients can flow\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Enable input gradients for 4-bit\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Re-enable gradient checkpointing (after disabling cache)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Tell Trainer which field is the label\n",
        "model.config.label_names = [\"labels\"]\n",
        "\n",
        "# Sanity check\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# -----------------------------------------\n",
        "# 5) Tokenize\n",
        "# -----------------------------------------\n",
        "def preprocess(examples):\n",
        "    prompts = [\n",
        "        f\"User: {inp}\\n\\nAssistant: {out}\"\n",
        "        for inp, out in zip(examples[\"input\"], examples[\"output\"])\n",
        "    ]\n",
        "    tok = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tok[\"labels\"] = tok[\"input_ids\"].clone()\n",
        "    return tok\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 6) Define Metrics Callback\n",
        "# -----------------------------------------\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.train_perplexities = []\n",
        "        self.learning_rates = []\n",
        "        self.epoch_metrics = {}\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is None: return\n",
        "        if \"loss\" in logs:\n",
        "            step, loss = state.global_step, logs[\"loss\"]\n",
        "            self.train_losses.append((step, loss))\n",
        "            self.train_perplexities.append((step, np.exp(loss)))\n",
        "        if \"learning_rate\" in logs:\n",
        "            self.learning_rates.append((state.global_step, logs[\"learning_rate\"]))\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        self.current_epoch = state.epoch\n",
        "        logger.info(f\"Epoch {self.current_epoch} start\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        losses = [\n",
        "            l for s, l in self.train_losses\n",
        "            if (self.current_epoch - 1) <= s / state.max_steps * args.num_train_epochs < self.current_epoch\n",
        "        ]\n",
        "        if losses:\n",
        "            avg = float(np.mean(losses))\n",
        "            self.epoch_metrics[self.current_epoch] = {\n",
        "                \"loss\": avg,\n",
        "                \"perplexity\": float(np.exp(avg)),\n",
        "                \"step\": state.global_step\n",
        "            }\n",
        "            logger.info(\n",
        "                f\"Epoch {self.current_epoch} end — loss: {avg:.4f}, \"\n",
        "                f\"perplexity: {np.exp(avg):.4f}\"\n",
        "            )\n",
        "\n",
        "# -----------------------------------------\n",
        "# 7) Trainer Setup\n",
        "# -----------------------------------------\n",
        "output_dir = \"/content/drive/MyDrive/fine_tuned_phi2_peft\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=default_data_collator,\n",
        "    callbacks=[MetricsCallback()],\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 8) Train & Save\n",
        "# -----------------------------------------\n",
        "logger.info(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "logger.info(\"Training complete; saving model & tokenizer\")\n",
        "model.save_pretrained(f\"{output_dir}/model\")\n",
        "tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# (Optional) 9) Visualization & Evaluation Helpers\n",
        "# -----------------------------------------\n",
        "def plot_training_curves(metrics_callback):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    steps, losses = zip(*metrics_callback.train_losses) if metrics_callback.train_losses else ([], [])\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(steps, losses); plt.xlabel('Step'); plt.ylabel('Loss'); plt.title('Training Loss'); plt.grid(True)\n",
        "    steps, perps = zip(*metrics_callback.train_perplexities) if metrics_callback.train_perplexities else ([], [])\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(steps, perps); plt.xlabel('Step'); plt.ylabel('Perplexity'); plt.title('Training Perplexity'); plt.grid(True)\n",
        "    if metrics_callback.learning_rates:\n",
        "        steps, lrs = zip(*metrics_callback.learning_rates)\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.plot(steps, lrs); plt.xlabel('Step'); plt.ylabel('LR'); plt.title('Learning Rate'); plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/training_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_epoch_metrics(metrics_callback):\n",
        "    if not metrics_callback.epoch_metrics:\n",
        "        return\n",
        "    epochs = sorted(metrics_callback.epoch_metrics)\n",
        "    losses = [metrics_callback.epoch_metrics[e][\"loss\"] for e in epochs]\n",
        "    perps = [metrics_callback.epoch_metrics[e][\"perplexity\"] for e in epochs]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, losses, marker='o'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss per Epoch'); plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, perps, marker='o'); plt.xlabel('Epoch'); plt.ylabel('Perplexity'); plt.title('PPL per Epoch'); plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/epoch_metrics.png\")\n",
        "    plt.close()\n",
        "\n",
        "def export_epoch_metrics(metrics_callback):\n",
        "    if not metrics_callback.epoch_metrics:\n",
        "        return\n",
        "    rows = []\n",
        "    for e, m in metrics_callback.epoch_metrics.items():\n",
        "        rows.append({'epoch': e, 'loss': m['loss'], 'perplexity': m['perplexity'], 'step': m['step']})\n",
        "    pd.DataFrame(rows).to_csv(f\"{output_dir}/epoch_metrics.csv\", index=False)\n",
        "    with open(f\"{output_dir}/epoch_metrics.json\", \"w\") as f:\n",
        "        json.dump(metrics_callback.epoch_metrics, f, indent=2)\n",
        "    logger.info(f\"Exported epoch metrics to CSV and JSON\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P3TSgNPZQNV"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import easyocr\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import gc\n",
        "import peft\n",
        "from peft import PeftModel\n",
        "from peft import PeftModelForCausalLM, PeftConfig\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"AR Food Label Health Analyzer\",\n",
        "    page_icon=\"🍞\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Title and introduction\n",
        "st.title(\"AR Food Label Health Analyzer\")\n",
        "st.markdown(\"\"\"\n",
        "This app analyzes food label ingredients based on your health profile to assess potential risks.\n",
        "Upload an image of a food label to get started. The AR overlay will highlight ingredients based on risk levels.\n",
        "\"\"\")\n",
        "\n",
        "# ----------------------- Configuration -----------------------\n",
        "# Sidebar for profile selection\n",
        "st.sidebar.header(\"User Profile\")\n",
        "USER_PROFILE = st.sidebar.selectbox(\n",
        "    \"Select your health profile:\",\n",
        "    [\"general\", \"diabetic\", \"hypertension\", \"celiac\", \"keto\", \"pregnancy\"]\n",
        ")\n",
        "\n",
        "# API keys input (for future expansion)\n",
        "st.sidebar.header(\"API Configuration\")\n",
        "GOOGLE_API_KEY = st.sidebar.text_input(\"Google API Key\", value=\"\", type=\"password\")\n",
        "GOOGLE_CX = st.sidebar.text_input(\"Google Custom Search Engine ID\", value=\"\")\n",
        "\n",
        "# Model configuration\n",
        "st.sidebar.header(\"Model Configuration\")\n",
        "MODEL_PATH = st.sidebar.text_input(\"Fine-tuned Model Path\", value=\"/content/drive/MyDrive/fine_tuned_phi2_peft\")\n",
        "MODEL_TEMPERATURE = st.sidebar.slider(\"Model temperature\", min_value=0.1, max_value=1.0, value=0.3, step=0.1)\n",
        "MAX_TOKEN_LENGTH = 512\n",
        "\n",
        "# Add debug mode toggle\n",
        "DEBUG_MODE = st.sidebar.checkbox(\"Enable Debug Mode\", value=False)\n",
        "\n",
        "# AR Configuration\n",
        "st.sidebar.header(\"AR Overlay Settings\")\n",
        "SHOW_AR_OVERLAY = st.sidebar.checkbox(\"Enable AR Overlay\", value=True)\n",
        "OVERLAY_OPACITY = st.sidebar.slider(\"Overlay Opacity\", min_value=0.1, max_value=1.0, value=0.5, step=0.1)\n",
        "HIGHLIGHT_MODE = st.sidebar.selectbox(\n",
        "    \"Highlight Method\",\n",
        "    [\"Box Highlight\", \"Text Highlight\", \"Connected Labels\"]\n",
        ")\n",
        "\n",
        "# Device detection\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "st.sidebar.write(f\"Using device: {'GPU' if DEVICE == 0 else 'CPU'}\")\n",
        "\n",
        "# ----------------------- Health Profile Definitions -----------------------\n",
        "HEALTH_PROFILES = {\n",
        "    \"diabetic\": {\n",
        "        \"concerns\": [\"blood sugar impact\", \"glycemic index\", \"carbohydrate content\", \"sugar content\"],\n",
        "        \"avoid\": [\"sugars\", \"high fructose corn syrup\", \"white flour\", \"dextrose\"],\n",
        "        \"monitor\": [\"carbohydrates\", \"starches\", \"maltodextrin\"]\n",
        "    },\n",
        "    \"hypertension\": {\n",
        "        \"concerns\": [\"sodium content\", \"blood pressure impact\", \"vasoactive compounds\"],\n",
        "        \"avoid\": [\"sodium\", \"salt\", \"MSG\", \"sodium phosphate\", \"sodium benzoate\"],\n",
        "        \"monitor\": [\"potassium sorbate\", \"preservatives\", \"nitrates\", \"nitrites\"]\n",
        "    },\n",
        "    \"celiac\": {\n",
        "        \"concerns\": [\"gluten content\", \"cross-contamination\", \"grain derivatives\"],\n",
        "        \"avoid\": [\"wheat\", \"barley\", \"rye\", \"malt\", \"brewer's yeast\", \"triticale\"],\n",
        "        \"monitor\": [\"oats\", \"modified food starch\", \"dextrin\", \"maltodextrin\"]\n",
        "    },\n",
        "    \"keto\": {\n",
        "        \"concerns\": [\"carbohydrate content\", \"sugar alcohols\", \"net carbs\"],\n",
        "        \"avoid\": [\"sugars\", \"starches\", \"flours\", \"corn syrup\"],\n",
        "        \"monitor\": [\"sugar alcohols\", \"fiber\", \"artificial sweeteners\"]\n",
        "    },\n",
        "    \"general\": {\n",
        "        \"concerns\": [\"additives\", \"preservatives\", \"artificial colors\", \"ultra-processed ingredients\"],\n",
        "        \"avoid\": [],\n",
        "        \"monitor\": [\"artificial colors\", \"artificial flavors\", \"preservatives\", \"high fructose corn syrup\"]\n",
        "    },\n",
        "    \"pregnancy\": {\n",
        "        \"concerns\": [\"fetal development\", \"toxins and contaminants\", \"nutrient absorption\", \"blood pressure impact\", \"blood sugar impact\"],\n",
        "        \"avoid\": [\"alcohol\", \"caffeine\", \"nitrates\", \"nitrites\", \"licorice root\", \"saccharin\", \"cyclamate\"],\n",
        "        \"monitor\": [\"sodium\", \"sugar\", \"artificial sweeteners\", \"preservatives\", \"food colorings\", \"MSG\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ----------------------- Model Loading -----------------------\n",
        "@st.cache_resource\n",
        "def load_model(model_path):\n",
        "    try:\n",
        "        # Define the base model name\n",
        "        base_model_name = \"microsoft/phi-2\"\n",
        "\n",
        "        # Load the tokenizer\n",
        "        tokenizer_path = os.path.join(model_path, \"tokenizer\")\n",
        "        if os.path.exists(tokenizer_path):\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "            if DEBUG_MODE:\n",
        "                st.sidebar.write(\"✅ Tokenizer loaded successfully\")\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "            if DEBUG_MODE:\n",
        "                st.sidebar.write(\"⚠️ Using base model tokenizer\")\n",
        "\n",
        "        # Ensure pad token is set\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # For direct inference without pipeline\n",
        "        peft_config = PeftConfig.from_pretrained(os.path.join(model_path, \"model\"))\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"✅ PEFT config loaded: {peft_config.base_model_name_or_path}\")\n",
        "\n",
        "        # Load the base model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Load the PEFT adapter\n",
        "        adapter_path = os.path.join(model_path, \"model\")\n",
        "        if not os.path.exists(adapter_path):\n",
        "            raise FileNotFoundError(f\"PEFT adapter not found at {adapter_path}\")\n",
        "\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "        model.eval()  # Set to evaluation mode\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(\"✅ Model loaded successfully\")\n",
        "\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {str(e)}\")\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"❌ Model loading error: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# ----------------------- Functions -----------------------\n",
        "def generate_with_model(model, tokenizer, prompt, max_new_tokens=150, temperature=0.3):\n",
        "    \"\"\"Generate text directly with the model instead of using pipeline.\"\"\"\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        # Format input using chat template\n",
        "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # Decode and extract assistant's response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract just the assistant's response\n",
        "        if \"ASSISTANT:\" in generated_text:\n",
        "            assistant_response = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
        "        else:\n",
        "            assistant_response = generated_text\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"✅ Generated response length: {len(assistant_response)}\")\n",
        "\n",
        "        return assistant_response\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"❌ Generation error: {str(e)}\")\n",
        "        return \"Error generating response\"\n",
        "\n",
        "def create_risk_assessment_prompt(ingredient, user_profile):\n",
        "    \"\"\"Create a prompt for the model to analyze an ingredient.\"\"\"\n",
        "    profile_data = HEALTH_PROFILES.get(user_profile.lower(), HEALTH_PROFILES[\"general\"])\n",
        "\n",
        "    # Formatting the prompt for the fine-tuned model\n",
        "    prompt = f\"\"\"\n",
        "    Ingredient: {ingredient}, Category: FOOD_INGREDIENT,\n",
        "    Glycemic Index: UNKNOWN, Carbs per 100g: UNKNOWN,\n",
        "    Sodium per 100g: UNKNOWN, Health Concerns: Diabetic Risk, Hypertension Risk, Pregnancy Risk\n",
        "\n",
        "    Analyze this ingredient for {user_profile} health profile with concerns: {\", \".join(profile_data['concerns'])}\n",
        "    Items to avoid: {\", \".join(profile_data['avoid'])}\n",
        "    Items to monitor: {\", \".join(profile_data['monitor'])}\n",
        "\n",
        "    Provide a risk assessment with risk_level (SAFE, LOW, MODERATE, HIGH, or UNKNOWN),\n",
        "    effects, and recommendations.\n",
        "    \"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "def parse_model_response(response_text):\n",
        "    \"\"\"Parse the model's response to extract risk assessment information.\"\"\"\n",
        "    try:\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"Parsing response: {response_text[:100]}...\")\n",
        "\n",
        "        # Try to extract risk level\n",
        "        risk_patterns = [\n",
        "            r'(?:risk_level|risk level|Risk Level|Risk):\\s*(SAFE|LOW|MODERATE|HIGH|UNKNOWN)',\n",
        "            r'(?:risk_level|risk level|Risk Level|Risk).*?(SAFE|LOW|MODERATE|HIGH|UNKNOWN)',\n",
        "            r'(SAFE|LOW|MODERATE|HIGH|UNKNOWN)\\s*(?:risk|Risk)',\n",
        "        ]\n",
        "\n",
        "        risk_level = \"UNKNOWN\"\n",
        "        for pattern in risk_patterns:\n",
        "            risk_match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "            if risk_match:\n",
        "                risk_level = risk_match.group(1).upper()\n",
        "                break\n",
        "\n",
        "        # Fallback risk assessment based on keywords if no explicit risk level found\n",
        "        if risk_level == \"UNKNOWN\":\n",
        "            if re.search(r'(?:high risk|very dangerous|avoid|harmful|toxic)', response_text, re.IGNORECASE):\n",
        "                risk_level = \"HIGH\"\n",
        "            elif re.search(r'(?:moderate risk|some concern|caution|careful)', response_text, re.IGNORECASE):\n",
        "                risk_level = \"MODERATE\"\n",
        "            elif re.search(r'(?:low risk|minimal concern|generally safe)', response_text, re.IGNORECASE):\n",
        "                risk_level = \"LOW\"\n",
        "            elif re.search(r'(?:safe|no concern|healthy|beneficial)', response_text, re.IGNORECASE):\n",
        "                risk_level = \"SAFE\"\n",
        "\n",
        "        # Extract effects and recommendations using more flexible patterns\n",
        "        effects_patterns = [\n",
        "            r'(?:Effects|effects):\\s*(.+?)(?:\\.|\\n|$|For|Recommend)',\n",
        "            r'(?:Impact|impact|effects of).*?(?:include|are|is)?\\s*(.+?)(?:\\.|\\n|$)',\n",
        "        ]\n",
        "\n",
        "        effects = []\n",
        "        for pattern in effects_patterns:\n",
        "            effects_match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "            if effects_match:\n",
        "                effect_text = effects_match.group(1).strip()\n",
        "                effects = [effect_text]\n",
        "                break\n",
        "\n",
        "        if not effects:\n",
        "            effects = [\"Effects not specified in response\"]\n",
        "\n",
        "        # Extract recommendations\n",
        "        rec_patterns = [\n",
        "            r'(?:Recommendations|recommendations):\\s*(.+?)(?:\\.|\\n|$)',\n",
        "            r'(?:recommend|advised|should).*?(.+?)(?:\\.|\\n|$)',\n",
        "        ]\n",
        "\n",
        "        recommendations = []\n",
        "        for pattern in rec_patterns:\n",
        "            rec_match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "            if rec_match:\n",
        "                rec_text = rec_match.group(1).strip()\n",
        "                recommendations = [rec_text]\n",
        "                break\n",
        "\n",
        "        if not recommendations:\n",
        "            recommendations = [\"Consult with healthcare provider\"]\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"Parsed risk level: {risk_level}\")\n",
        "\n",
        "        return {\n",
        "            \"risk_level\": risk_level,\n",
        "            \"effects\": effects,\n",
        "            \"recommendations\": recommendations\n",
        "        }\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"❌ Parsing error: {str(e)}\")\n",
        "        return {\n",
        "            \"risk_level\": \"UNKNOWN\",\n",
        "            \"effects\": [\"Unable to determine effects\"],\n",
        "            \"recommendations\": [\"Consult with healthcare provider\"]\n",
        "        }\n",
        "\n",
        "def analyze_ingredient_risk(model, tokenizer, ingredient, user_profile):\n",
        "    \"\"\"Analyze the risk of an ingredient using the fine-tuned model.\"\"\"\n",
        "    try:\n",
        "        # Generate the risk analysis prompt\n",
        "        prompt = create_risk_assessment_prompt(ingredient, user_profile)\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"Analyzing ingredient: {ingredient}\")\n",
        "\n",
        "        # First try: specific prompt for fine-tuned model\n",
        "        generated_text = generate_with_model(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            prompt,\n",
        "            max_new_tokens=150,\n",
        "            temperature=MODEL_TEMPERATURE\n",
        "        )\n",
        "\n",
        "        # Parse the response\n",
        "        analysis = parse_model_response(generated_text)\n",
        "\n",
        "        # If still unknown, try a more general prompt\n",
        "        if analysis[\"risk_level\"] == \"UNKNOWN\":\n",
        "            fallback_prompt = f\"Is {ingredient} safe or risky for someone with {user_profile} health concerns? Respond with a risk level (SAFE, LOW, MODERATE, HIGH) and explain why.\"\n",
        "            generated_text = generate_with_model(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                fallback_prompt,\n",
        "                max_new_tokens=150,\n",
        "                temperature=MODEL_TEMPERATURE\n",
        "            )\n",
        "            analysis = parse_model_response(generated_text)\n",
        "\n",
        "        # Add hardcoded rules for common ingredients as fallback\n",
        "        if analysis[\"risk_level\"] == \"UNKNOWN\":\n",
        "            # Add common ingredient risk mappings\n",
        "            common_ingredients = {\n",
        "                \"wheat flour\": {\"diabetic\": \"MODERATE\", \"celiac\": \"HIGH\", \"general\": \"LOW\"},\n",
        "                \"sugar\": {\"diabetic\": \"HIGH\", \"keto\": \"HIGH\", \"general\": \"MODERATE\"},\n",
        "                \"salt\": {\"hypertension\": \"HIGH\", \"general\": \"LOW\"},\n",
        "                \"wheat\": {\"celiac\": \"HIGH\", \"general\": \"LOW\"},\n",
        "                \"yeast\": {\"general\": \"SAFE\"},\n",
        "                \"vegetable oil\": {\"general\": \"LOW\", \"keto\": \"MODERATE\"},\n",
        "                \"palm oil\": {\"general\": \"MODERATE\"},\n",
        "                \"preservative\": {\"general\": \"MODERATE\", \"pregnancy\": \"MODERATE\"},\n",
        "                \"emulsifier\": {\"general\": \"LOW\"},\n",
        "            }\n",
        "\n",
        "            # Check if ingredient is in our common list or contains any common ingredients\n",
        "            for common_ing, risk_map in common_ingredients.items():\n",
        "                if common_ing in ingredient.lower():\n",
        "                    analysis[\"risk_level\"] = risk_map.get(user_profile.lower(), risk_map.get(\"general\", \"UNKNOWN\"))\n",
        "                    analysis[\"effects\"] = [f\"Common {common_ing} effects for {user_profile} profile\"]\n",
        "                    analysis[\"recommendations\"] = [f\"Standard recommendations for {common_ing}\"]\n",
        "                    break\n",
        "\n",
        "        return analysis[\"risk_level\"], analysis\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            st.sidebar.write(f\"❌ Analysis error: {str(e)}\")\n",
        "        return \"UNKNOWN\", {\"effects\": [\"Error analyzing ingredient\"], \"recommendations\": [\"Consult healthcare provider\"]}\n",
        "\n",
        "def evaluate_overall_product_risk(assessments, user_profile):\n",
        "    \"\"\"Calculate overall product risk based on ingredient assessments.\"\"\"\n",
        "    risk_weights = {\n",
        "        \"HIGH\": 4,\n",
        "        \"MODERATE\": 2,\n",
        "        \"LOW\": 1,\n",
        "        \"SAFE\": 0,\n",
        "        \"UNKNOWN\": 1\n",
        "    }\n",
        "\n",
        "    if not assessments:\n",
        "        return \"UNKNOWN\", [\"Unable to assess without ingredients\"]\n",
        "\n",
        "    total_weight = 0\n",
        "    total_ingredients = len(assessments)\n",
        "\n",
        "    # Simple weighting - each ingredient contributes equally\n",
        "    for ingredient, (risk, _) in assessments.items():\n",
        "        total_weight += risk_weights[risk]\n",
        "\n",
        "    # Normalize by number of ingredients\n",
        "    if total_ingredients > 0:\n",
        "        normalized = total_weight / (total_ingredients * risk_weights[\"HIGH\"])\n",
        "    else:\n",
        "        return \"UNKNOWN\", [\"No ingredients to assess\"]\n",
        "\n",
        "    # Use different thresholds based on health profile\n",
        "    thresholds = {\n",
        "        \"diabetic\": [0.4, 0.6, 0.8],\n",
        "        \"hypertension\": [0.3, 0.5, 0.7],\n",
        "        \"celiac\": [0.2, 0.4, 0.6],\n",
        "        \"keto\": [0.5, 0.7, 0.85],\n",
        "        \"general\": [0.25, 0.5, 0.75],\n",
        "        \"pregnancy\": [0.3, 0.5, 0.7]\n",
        "    }\n",
        "\n",
        "    profile = user_profile.lower()\n",
        "    thr = thresholds.get(profile, thresholds[\"general\"])\n",
        "\n",
        "    if normalized >= thr[2]:\n",
        "        overall = \"HIGH\"\n",
        "        recs = [\"This product poses significant risks based on your health profile. Consider alternatives.\"]\n",
        "    elif normalized >= thr[1]:\n",
        "        overall = \"MODERATE\"\n",
        "        recs = [\"This product contains some concerning ingredients. Consume in moderation and monitor your response.\"]\n",
        "    elif normalized >= thr[0]:\n",
        "        overall = \"LOW\"\n",
        "        recs = [\"This product appears relatively safe but contains ingredients to monitor.\"]\n",
        "    else:\n",
        "        overall = \"SAFE\"\n",
        "        recs = [\"This product appears safe based on your health profile.\"]\n",
        "\n",
        "    return overall, recs\n",
        "\n",
        "# -------------- OCR and AR OVERLAY FUNCTIONS ------------------\n",
        "def extract_text_from_image(image):\n",
        "    \"\"\"Extract text from an image using EasyOCR.\"\"\"\n",
        "    reader = easyocr.Reader(['en'], gpu=(DEVICE == 0))\n",
        "    if isinstance(image, str):  # If image is a path\n",
        "        result = reader.readtext(image)\n",
        "    else:  # If image is an array/PIL Image\n",
        "        result = reader.readtext(np.array(image))\n",
        "    return result  # Return full OCR result with bounding boxes\n",
        "\n",
        "def parse_ingredients(ocr_result):\n",
        "    \"\"\"Parse ingredients list from OCR results.\"\"\"\n",
        "    # Extract full text first\n",
        "    full_text = \" \".join([detection[1] for detection in ocr_result])\n",
        "\n",
        "    # Find the ingredients section\n",
        "    pattern = re.compile(r'(?i)ingredients?:?\\s*(.*)', re.IGNORECASE | re.DOTALL)\n",
        "    match = pattern.search(full_text)\n",
        "    ingredients_str = match.group(1).strip() if match else full_text.strip()\n",
        "\n",
        "    # Cut off at certain keywords\n",
        "    cutoffs = [r'(?i)\\ballergen', r'(?i)\\bnutrition', r'(?i)\\bwarning', r'(?i)\\bstorage', r'(?i)\\bmanufactured', r'(?i)\\bmay contain']\n",
        "    for c in cutoffs:\n",
        "        c_re = re.compile(c)\n",
        "        c_match = c_re.search(ingredients_str)\n",
        "        if c_match:\n",
        "            ingredients_str = ingredients_str[:c_match.start()]\n",
        "            break\n",
        "\n",
        "    ingredients_str = ingredients_str.strip()\n",
        "\n",
        "    # Process ingredients\n",
        "    results = []\n",
        "\n",
        "    # Split by common separators\n",
        "    separators = [',', '.', ';', '•']\n",
        "    for sep in separators:\n",
        "        if sep in ingredients_str:\n",
        "            ingredients = [i.strip() for i in ingredients_str.split(sep) if i.strip()]\n",
        "            if len(ingredients) > 1:\n",
        "                results = ingredients\n",
        "                break\n",
        "\n",
        "    # If no separators found, try to use the full string\n",
        "    if not results and ingredients_str:\n",
        "        results = [ingredients_str]\n",
        "\n",
        "    # Clean up ingredients (remove numbers at start, etc.)\n",
        "    cleaned_results = []\n",
        "    for ingredient in results:\n",
        "        # Remove numbering\n",
        "        cleaned = re.sub(r'^\\d+[\\.\\)]\\s*', '', ingredient).strip()\n",
        "        if cleaned:\n",
        "            cleaned_results.append(cleaned)\n",
        "\n",
        "    return cleaned_results\n",
        "\n",
        "def create_ar_overlay(img, ocr_result, ingredient_assessments):\n",
        "    \"\"\"Create AR overlay on the image highlighting ingredients based on risk levels.\"\"\"\n",
        "    # Create a copy of the image for overlay\n",
        "    img_pil = Image.fromarray(np.array(img))\n",
        "    overlay = Image.new('RGBA', img_pil.size, (0, 0, 0, 0))\n",
        "    draw = ImageDraw.Draw(overlay)\n",
        "\n",
        "    # Risk color mapping\n",
        "    risk_colors = {\n",
        "        \"HIGH\": (255, 0, 0, int(255 * OVERLAY_OPACITY)),        # Red\n",
        "        \"MODERATE\": (255, 165, 0, int(255 * OVERLAY_OPACITY)),  # Orange\n",
        "        \"LOW\": (255, 255, 0, int(255 * OVERLAY_OPACITY)),       # Yellow\n",
        "        \"SAFE\": (0, 255, 0, int(255 * OVERLAY_OPACITY)),        # Green\n",
        "        \"UNKNOWN\": (128, 128, 128, int(255 * OVERLAY_OPACITY))  # Gray\n",
        "    }\n",
        "\n",
        "    # Font setup for labels\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Match detected text with assessed ingredients\n",
        "    for detection in ocr_result:\n",
        "        bbox, text, _ = detection\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Check if this text box contains an ingredient\n",
        "        matched_ingredient = None\n",
        "        for ingredient, (risk_level, details) in ingredient_assessments.items():\n",
        "            # Improved matching algorithm\n",
        "            ingredient_lower = ingredient.lower()\n",
        "            # Check if ingredient is a substring of text or vice versa,\n",
        "            # or if there's substantial overlap (more than 70% match)\n",
        "            if (ingredient_lower in text_lower or\n",
        "                text_lower in ingredient_lower or\n",
        "                (len(set(ingredient_lower.split()) & set(text_lower.split())) /\n",
        "                 max(len(set(ingredient_lower.split())), len(set(text_lower.split()))) > 0.7)):\n",
        "                matched_ingredient = ingredient\n",
        "                risk = risk_level\n",
        "                break\n",
        "\n",
        "        if matched_ingredient:\n",
        "            # Convert points to rectangle\n",
        "            box_points = np.array(bbox).astype(np.int32)\n",
        "            x0, y0 = min(box_points[:, 0]), min(box_points[:, 1])\n",
        "            x1, y1 = max(box_points[:, 0]), max(box_points[:, 1])\n",
        "\n",
        "            # Apply different highlight methods based on settings\n",
        "            if HIGHLIGHT_MODE == \"Box Highlight\":\n",
        "                # Draw filled rectangle with transparency\n",
        "                draw.rectangle([x0, y0, x1, y1], fill=risk_colors[risk])\n",
        "\n",
        "            elif HIGHLIGHT_MODE == \"Text Highlight\":\n",
        "                # Draw outline\n",
        "                draw.rectangle([x0, y0, x1, y1], outline=risk_colors[risk][:3] + (255,), width=2)\n",
        "\n",
        "                # Add risk label\n",
        "                label = f\"{risk}\"\n",
        "                draw.text((x0, y0-20), label, fill=risk_colors[risk][:3] + (255,), font=font)\n",
        "\n",
        "            elif HIGHLIGHT_MODE == \"Connected Labels\":\n",
        "                # Draw connecting line\n",
        "                margin = 50\n",
        "                label_x = x1 + 10\n",
        "                label_y = (y0 + y1) // 2\n",
        "\n",
        "                # Draw line\n",
        "                draw.line([(x1, label_y), (label_x + margin, label_y)],\n",
        "                          fill=risk_colors[risk][:3] + (255,), width=2)\n",
        "\n",
        "                # Draw label box\n",
        "                label_width = 80\n",
        "                label_height = 40\n",
        "                draw.rectangle(\n",
        "                    [label_x + margin, label_y - label_height//2,\n",
        "                     label_x + margin + label_width, label_y + label_height//2],\n",
        "                    fill=risk_colors[risk])\n",
        "\n",
        "                # Add text\n",
        "                draw.text((label_x + margin + 5, label_y - 7), f\"{risk}\",\n",
        "                          fill=(255, 255, 255, 255), font=font)\n",
        "\n",
        "    # Combine original image with overlay\n",
        "    result = Image.alpha_composite(img_pil.convert('RGBA'), overlay)\n",
        "    return result\n",
        "\n",
        "def generate_risk_summary(ingredient_assessments, overall_risk, recommendations):\n",
        "    \"\"\"Generate a pandas DataFrame summarizing risk assessments.\"\"\"\n",
        "    summary = pd.DataFrame(columns=[\"Ingredient\", \"Risk Level\", \"Effects\", \"Recommendations\"])\n",
        "\n",
        "    for ingredient, (risk_level, details) in ingredient_assessments.items():\n",
        "        effects = \"; \".join(details.get(\"effects\", []))\n",
        "        recs = \"; \".join(details.get(\"recommendations\", []))\n",
        "\n",
        "        summary = pd.concat([summary, pd.DataFrame({\n",
        "            \"Ingredient\": [ingredient],\n",
        "            \"Risk Level\": [risk_level],\n",
        "            \"Effects\": [effects],\n",
        "            \"Recommendations\": [recs]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# ----------------------- Main Application Logic -----------------------\n",
        "def main():\n",
        "    # Load the model and tokenizer\n",
        "    model, tokenizer = load_model(MODEL_PATH)\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        st.error(\"Failed to load the fine-tuned model. Please check the model path and try again.\")\n",
        "        return\n",
        "\n",
        "    tabs = st.tabs([\"Upload & Analyze\", \"Results Dashboard\", \"About\"])\n",
        "\n",
        "    with tabs[0]:\n",
        "        st.header(\"Upload Food Label Image\")\n",
        "        uploaded_file = st.file_uploader(\"Choose an image of a food label...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "        if uploaded_file is not None:\n",
        "            # Process the uploaded image\n",
        "            image = Image.open(uploaded_file)\n",
        "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "            with st.spinner(\"Processing image...\"):\n",
        "                # Get OCR results\n",
        "                ocr_result = extract_text_from_image(image)\n",
        "\n",
        "                # Extract ingredients list\n",
        "                ingredients = parse_ingredients(ocr_result)\n",
        "\n",
        "                # Display extracted ingredients\n",
        "                st.subheader(\"Extracted Ingredients\")\n",
        "                for i, ingredient in enumerate(ingredients):\n",
        "                    st.write(f\"{i+1}. {ingredient}\")\n",
        "\n",
        "                # Initialize ingredient assessments\n",
        "                ingredient_assessments = {}\n",
        "                progress_bar = st.progress(0)\n",
        "\n",
        "                for i, ingredient in enumerate(ingredients):\n",
        "                    # Get risk assessment\n",
        "                    risk_level, details = analyze_ingredient_risk(model, tokenizer, ingredient, USER_PROFILE)\n",
        "                    ingredient_assessments[ingredient] = (risk_level, details)\n",
        "\n",
        "                    # Update progress\n",
        "                    progress_bar.progress((i + 1) / len(ingredients))\n",
        "                    time.sleep(0.1)  # Small delay to prevent rate limiting\n",
        "\n",
        "                # Evaluate overall product risk\n",
        "                overall_risk, recommendations = evaluate_overall_product_risk(ingredient_assessments, USER_PROFILE)\n",
        "\n",
        "                # Store results in session state for the dashboard\n",
        "                st.session_state['ingredient_assessments'] = ingredient_assessments\n",
        "                st.session_state['overall_risk'] = overall_risk\n",
        "                st.session_state['recommendations'] = recommendations\n",
        "                st.session_state['original_image'] = image\n",
        "                st.session_state['ocr_result'] = ocr_result\n",
        "\n",
        "                # Create AR overlay\n",
        "                if SHOW_AR_OVERLAY:\n",
        "                    ar_image = create_ar_overlay(image, ocr_result, ingredient_assessments)\n",
        "                    st.session_state['ar_image'] = ar_image\n",
        "\n",
        "                # Show success message\n",
        "                st.success(\"Analysis complete! Check the Results Dashboard for details.\")\n",
        "\n",
        "    with tabs[1]:\n",
        "        st.header(\"Analysis Results\")\n",
        "\n",
        "        if 'ingredient_assessments' in st.session_state:\n",
        "            # Display overall risk rating\n",
        "            overall_risk = st.session_state['overall_risk']\n",
        "            recommendations = st.session_state['recommendations']\n",
        "\n",
        "            # Display risk with appropriate styling\n",
        "            risk_colors = {\n",
        "                \"HIGH\": \"#FF0000\",\n",
        "                \"MODERATE\": \"#FFA500\",\n",
        "                \"LOW\": \"#FFFF00\",\n",
        "                \"SAFE\": \"#00FF00\",\n",
        "                \"UNKNOWN\": \"#808080\"\n",
        "            }\n",
        "\n",
        "            st.markdown(f\"\"\"\n",
        "            <div style=\"padding: 20px; border-radius: 10px; background-color: {risk_colors.get(overall_risk, '#808080')};\">\n",
        "                <h2 style=\"color: white; text-align: center;\">Overall Risk: {overall_risk}</h2>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            # Display recommendations\n",
        "            st.subheader(\"Recommendations\")\n",
        "            for rec in recommendations:\n",
        "                st.markdown(f\"- {rec}\")\n",
        "\n",
        "            # Display AR overlay\n",
        "            if 'ar_image' in st.session_state:\n",
        "                st.subheader(\"AR Health Analysis Overlay\")\n",
        "                st.image(st.session_state['ar_image'], caption=\"AR Overlay\", use_column_width=True)\n",
        "\n",
        "            # Display detailed ingredient analysis\n",
        "            st.subheader(\"Detailed Ingredient Analysis\")\n",
        "            summary_df = generate_risk_summary(st.session_state['ingredient_assessments'], overall_risk, recommendations)\n",
        "            st.dataframe(summary_df)\n",
        "\n",
        "            # Add visualization\n",
        "            st.subheader(\"Risk Distribution\")\n",
        "            risk_counts = {\n",
        "                \"HIGH\": 0,\n",
        "                \"MODERATE\": 0,\n",
        "                \"LOW\": 0,\n",
        "                \"SAFE\": 0,\n",
        "                \"UNKNOWN\": 0\n",
        "            }\n",
        "\n",
        "            for _, (risk, _) in st.session_state['ingredient_assessments'].items():\n",
        "                risk_counts[risk] += 1\n",
        "\n",
        "            # Create pie chart\n",
        "            fig, ax = plt.subplots()\n",
        "            labels = [f\"{k} ({v})\" for k, v in risk_counts.items() if v > 0]\n",
        "            sizes = [v for v in risk_counts.values() if v > 0]\n",
        "            colors = [risk_colors[k] for k, v in risk_counts.items() if v > 0]\n",
        "\n",
        "            if sizes:  # Only create pie chart if there are values\n",
        "                wedges, texts = ax.pie(sizes, colors=colors, startangle=90, wedgeprops={'alpha': 0.7})\n",
        "                ax.legend(wedges, labels, title=\"Risk Levels\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "                st.pyplot(fig)\n",
        "            else:\n",
        "                st.write(\"No risk data available to display.\")\n",
        "\n",
        "            # Export functionality\n",
        "            st.download_button(\n",
        "                label=\"Download Analysis as CSV\",\n",
        "                data=summary_df.to_csv(index=False),\n",
        "                file_name=\"food_label_analysis.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "        else:\n",
        "            st.info(\"No analysis results yet. Please upload an image in the Upload & Analyze tab.\")\n",
        "\n",
        "    with tabs[2]:\n",
        "        st.header(\"About AR Food Label Health Analyzer\")\n",
        "        st.markdown(\"\"\"\n",
        "        ### How It Works\n",
        "\n",
        "        This application uses computer vision and AI to analyze food labels and provide health recommendations based on your specific health profile:\n",
        "\n",
        "        1. **Upload**: Take a photo of any food product ingredient list\n",
        "        2. **OCR Processing**: The system extracts text from the image\n",
        "        3. **AI Analysis**: Each ingredient is evaluated against health criteria using our fine-tuned AI model\n",
        "        4. **AR Overlay**: Visual indicators show risk levels directly on the image\n",
        "        5. **Custom Recommendations**: Get personalized advice for your health profile\n",
        "\n",
        "        ### Health Profiles\n",
        "\n",
        "        The analyzer supports multiple health profiles:\n",
        "        - **General**: Focus on additives and ultra-processed ingredients\n",
        "        - **Diabetic**: Analyze glycemic impact and carbohydrate content\n",
        "        - **Hypertension**: Evaluate sodium content and blood pressure impact\n",
        "        - **Celiac**: Identify gluten risks and cross-contamination concerns\n",
        "        - **Keto**: Assess carbohydrate content for ketogenic diet compatibility\n",
        "        - **Pregnancy**: Identify ingredients with potential risks during pregnancy\n",
        "\n",
        "        ### Privacy & Data\n",
        "\n",
        "        Your images and analysis results are not stored after you close the session. All processing occurs locally within the application.\n",
        "        \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7ufc6DgCuBY"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 2wRR3D8n9Xvbexgw5PUnN1tUOiG_2ndhhe77DvezhBHL2Pish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8gv_YxNFtd-"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "\n",
        "# Kill previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Set port for Streamlit\n",
        "port = 8501\n",
        "\n",
        "# Start Streamlit in a background thread\n",
        "def run_streamlit():\n",
        "    os.system(f\"streamlit run app.py --server.port {port}\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Wait a bit for Streamlit to initialize\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Create the public tunnel\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"✅ Streamlit app URL: {public_url}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}